{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmiBhy51Ib50",
    "colab_type": "text"
   },
   "source": [
    "Установим нужные нам версии библиотек:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0Hbfzq-AIP1T",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "!pip install keras==2.2.2\n",
    "!pip install tensorflow==1.15.0\n",
    "!pip install keras_applications==1.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD6bjv5OIjmi",
    "colab_type": "text"
   },
   "source": [
    "Подключимся к гугл-диску, чтобы получить доступ к датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_-JJ6eU83VVF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYhQBrUeK86J",
    "colab_type": "text"
   },
   "source": [
    "Распакуем архив с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nSj02GY74cVd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "time_dir = '/content/gdrive/My Drive/Time_extraction/'\n",
    "\n",
    "if 'train' in os.listdir(time_dir):\n",
    "  print('Files are already extracted')\n",
    "else:\n",
    "  with zipfile.ZipFile(os.path.join(time_dir, 'train.zip'), 'r') as zip_ref:\n",
    "      zip_ref.extractall(os.path.join(time_dir))\n",
    "\n",
    "print(len(os.listdir(os.path.join(time_dir, 'train'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmNYBPS9LcJf",
    "colab_type": "text"
   },
   "source": [
    "Импортируем нужные нам библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aZYhudC58zPC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "dzt4FOeNCfTa",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl5UxAIAL3Ha",
    "colab_type": "text"
   },
   "source": [
    "Определим класс для загрузки наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "AeMyk7MO5ubs",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "\n",
    "    def __init__(self, path_to_data_dir):\n",
    "        self._path_to_data_dir = path_to_data_dir\n",
    "        self._data_files = sorted(os.listdir(self._path_to_data_dir))\n",
    "\n",
    "\n",
    "    def load_dataset(self):\n",
    "        sentences = []\n",
    "        for data_file in tqdm(self._data_files, desc='Loading data'):\n",
    "            if not data_file.endswith('.csv'):\n",
    "              continue\n",
    "            with open(os.path.join(self._path_to_data_dir, data_file), 'r') as data_f:\n",
    "                reader = csv.DictReader(data_f)\n",
    "                sentence = []\n",
    "                for row in reader:\n",
    "                    sentence.append((row['token'], row['tag']))\n",
    "                sentences.append(sentence)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yErEXMp6MJ3l",
    "colab_type": "text"
   },
   "source": [
    "Определим класс для векторизации данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zcewqi5c588E",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._max_sentence_len = 1000\n",
    "        self._max_wordform_len = 30\n",
    "        self._all_chars = u'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM,.?!:;\"«»-—1234567890'\n",
    "        self._len_all_chars = len(self._all_chars)\n",
    "\n",
    "        self._tag2id = {'O': 0, 'B-TIME': 1, 'I-TIME': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-DURATION': 5, 'I-DURATION': 6,\n",
    "                        'B-SET': 7, 'I-SET': 8}\n",
    "    \n",
    "    def get_len_all_chars(self):\n",
    "      return self._len_all_chars\n",
    "\n",
    "    def vectorize_chars(self, sentences):\n",
    "        X_chars = self.vectorize_chars_dataset(sentences)\n",
    "        return X_chars\n",
    "\n",
    "    def vectorize_targets(self, sentences):\n",
    "        y = [[self._tag2id[w[1]] for w in s] for s in sentences]\n",
    "        y = pad_sequences(maxlen=self._max_sentence_len, sequences=y, padding=\"post\", value=self._tag2id[\"O\"])\n",
    "        y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "        return y\n",
    "\n",
    "    def vectorize_chars_dataset(self, sentences):\n",
    "        X_chars = [[self._vectorize_chars_wordform(w[0]) for w in s] for s in sentences]\n",
    "        X_chars = pad_sequences(maxlen=self._max_sentence_len, sequences=X_chars, padding=\"post\",\n",
    "                                value=np.zeros((self._max_wordform_len, self._len_all_chars), dtype=np.int32))\n",
    "        return X_chars\n",
    "\n",
    "    def _vectorize_chars_wordform(self, wordform):\n",
    "        vector = np.zeros(self._max_wordform_len * self._len_all_chars)\n",
    "        for i in range(len(wordform)):\n",
    "            if i == self._max_wordform_len:\n",
    "                break\n",
    "            if wordform[i] in self._all_chars:\n",
    "                ind = self._all_chars.index(wordform[i])\n",
    "                vector[i * self._len_all_chars + ind] = 1.0\n",
    "        vector = vector.reshape((self._max_wordform_len, self._len_all_chars))\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up-mBrVpNBdj",
    "colab_type": "text"
   },
   "source": [
    "Определим класс для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7ihV_UTW6MYV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self):\n",
    "        data_dir = os.path.join(time_dir, 'train')\n",
    "        self._data_loader = DatasetLoader(data_dir)\n",
    "        self._vectorizer = Vectorizer()\n",
    "\n",
    "        self._sentences = self._data_loader.load_dataset()\n",
    "        self._val_sentences = self._sentences[-50:]\n",
    "        self._train_samples = self._sentences[:200]\n",
    "\n",
    "        self._batch_size = 5\n",
    "        self._max_len = 1000\n",
    "\n",
    "        self._model = self._define_model()\n",
    "\n",
    "    def _define_model(self):\n",
    "        input_chars = Input(shape=(self._max_len, 30, self._vectorizer.get_len_all_chars()))\n",
    "        chars = TimeDistributed(Bidirectional(LSTM(units=100,\n",
    "                                                   recurrent_dropout=0.5)))(input_chars)\n",
    "\n",
    "        crf = CRF(9, sparse_target=True)  # CRF layer\n",
    "        out = crf(chars)  # output\n",
    "\n",
    "        model = Model([input_chars], out)\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "        return model\n",
    "\n",
    "    def _generate_train_samples(self):\n",
    "      i = 0\n",
    "      while True:\n",
    "        X_chars = self._vectorizer.vectorize_chars(self._train_samples[i:i + self._batch_size])\n",
    "        y = self._vectorizer.vectorize_targets(self._train_samples[i:i + self._batch_size])\n",
    "        i += self._batch_size\n",
    "        yield [np.array(X_chars)], np.array(y)\n",
    "        if i == len(self._train_samples):\n",
    "          i = 0\n",
    "\n",
    "    def _generate_val_samples(self):\n",
    "      i = 0\n",
    "      while True:\n",
    "        X_chars = self._vectorizer.vectorize_chars(self._val_sentences[i:i + self._batch_size])\n",
    "        y = self._vectorizer.vectorize_targets(self._val_sentences[i:i + self._batch_size])\n",
    "        i += self._batch_size\n",
    "        yield [np.array(X_chars)], np.array(y)\n",
    "        if i == len(self._val_sentences):\n",
    "          i = 0\n",
    "\n",
    "    def train(self):\n",
    "        weights_dir = '/content/gdrive/My Drive/Time_extraction/weights'\n",
    "        if not os.path.exists(weights_dir):\n",
    "          os.makedirs(weights_dir)\n",
    "        weights_file = 'weights_chars_crf.h5'\n",
    "        modelPath = os.path.join(weights_dir, weights_file)\n",
    "        saver = ModelCheckpoint(modelPath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "        stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "        history = self._model.fit_generator(self._generate_train_samples(), epochs=50,\n",
    "                            validation_data=self._generate_val_samples(), steps_per_epoch=40, validation_steps=10,\n",
    "                            verbose=1, callbacks=[saver, stopper])\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eJSQXKqA7xgl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmJ39h4A2WQU",
    "colab_type": "text"
   },
   "source": [
    "Теперь давайте попробуем найти временные выражения, используя обученную модель. Для этого напишем класс Predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QRiid9hLpDax",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._vectorizer = Vectorizer()\n",
    "        self._trainer = Trainer()\n",
    "        self._tokenizer = WordPunctTokenizer()\n",
    "        self._model = self._trainer._model\n",
    "        self._model.load_weights('/content/gdrive/My Drive/Time_extraction/weights/weights_chars_crf.h5')\n",
    "        self._tag2id = {'O': 0, 'B-TIME': 1, 'I-TIME': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-DURATION': 5, 'I-DURATION': 6,\n",
    "                        'B-SET': 7, 'I-SET': 8}\n",
    "        self._id2tag = self._get_id2tag()\n",
    "\n",
    "    def _get_id2tag(self):\n",
    "        id2tag = dict()\n",
    "        for tag, id in self._tag2id.items():\n",
    "            id2tag[id] = tag\n",
    "        return id2tag\n",
    "\n",
    "    def _process_input_sentence(self, sentence):\n",
    "        tokens = self._tokenizer.tokenize(sentence)\n",
    "        tokens_to_process = [(token, 'O') for token in tokens]\n",
    "        input_tokens = []\n",
    "        for i in range(20):\n",
    "            input_tokens.append(tokens_to_process)\n",
    "        X_chars = self._vectorizer.vectorize_chars(sentences=input_tokens)\n",
    "        return X_chars, tokens\n",
    "\n",
    "    def predict(self, text):\n",
    "        X_chars, tokens = self._process_input_sentence(text)\n",
    "        predicts = self._model.predict([X_chars])[0]\n",
    "        result = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            tag = self._id2tag[np.argmax(predicts[i])]\n",
    "            result.append((token, tag))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "PMNqOVNvp4S1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "predictor = Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zAx_mcUtqAcg",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "text = 'he was born in 1994'\n",
    "result = predictor.predict(text)\n",
    "for r in result:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Time_extraction_chars.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
