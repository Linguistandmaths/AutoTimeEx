{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"time_extraction_chars_embeddings_1_0.ipynb","provenance":[{"file_id":"10_v96j-LWVjQ8y660UuzVmInb57qeAMb","timestamp":1590512719134}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SD6bjv5OIjmi","colab_type":"text"},"source":["Подключимся к гугл-диску, чтобы получить доступ к датасету:"]},{"cell_type":"code","metadata":{"id":"_-JJ6eU83VVF","colab_type":"code","outputId":"a9c1737c-b5cd-4ae9-f66b-0ec55b973e44","executionInfo":{"status":"ok","timestamp":1591090193500,"user_tz":-420,"elapsed":25530,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZmiBhy51Ib50","colab_type":"text"},"source":["Установим нужные нам версии библиотек:\n","\n"]},{"cell_type":"code","metadata":{"id":"0Hbfzq-AIP1T","colab_type":"code","colab":{}},"source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git\n","!pip install keras==2.2.2\n","!pip install tensorflow==1.15.0\n","!pip install keras_applications==1.0.7"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxgVUsR9aI6c","colab_type":"text"},"source":["\n","\n","\n","\n","\n","\n","\n","Задаем пути "]},{"cell_type":"code","metadata":{"id":"4adXs1uwYhAm","colab_type":"code","colab":{}},"source":["time_dir = '/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/'\n","# имя папки, где будут результаты оценки и часть имени с весами\n","path_to_exp = '120_5_3_150_chars_elmo_2'\n","result_dir = '/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/test_ml/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZYhudC58zPC","colab_type":"code","outputId":"8c195036-a7fd-4cfa-b6f1-06be4f5aa7e0","executionInfo":{"status":"ok","timestamp":1591090323109,"user_tz":-420,"elapsed":127089,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import csv\n","\n","import os\n","import zipfile\n","import numpy as np\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import keras\n","from keras import backend as K\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model, Input\n","from keras.layers.merge import add, concatenate\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras_contrib.layers import CRF"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MYhQBrUeK86J","colab_type":"text"},"source":["Распакуем архив с данными:"]},{"cell_type":"code","metadata":{"id":"nSj02GY74cVd","colab_type":"code","outputId":"7aea6cb3-4c7d-4a5f-98c4-ba1160f6ed93","executionInfo":{"status":"error","timestamp":1590923472839,"user_tz":-420,"elapsed":2550,"user":{"displayName":"Лена Бручес","photoUrl":"","userId":"06616238160948350875"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["if 'train' in os.listdir(time_dir):\n","  print('Files are already extracted')\n","else:\n","  with zipfile.ZipFile(os.path.join(time_dir, 'train.zip'), 'r') as zip_ref:\n","      zip_ref.extractall(os.path.join(time_dir, 'train'))\n","\n","print(len(os.listdir(os.path.join(time_dir, 'train'))))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-3b259f1a44b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files are already extracted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/'"]}]},{"cell_type":"markdown","metadata":{"id":"JmNYBPS9LcJf","colab_type":"text"},"source":["Импортируем нужные нам библиотеки:"]},{"cell_type":"code","metadata":{"id":"dzt4FOeNCfTa","colab_type":"code","outputId":"68c4c6bb-1c17-496f-93e2-a070e1c8bcbc","executionInfo":{"status":"ok","timestamp":1591090324450,"user_tz":-420,"elapsed":1319,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(tf.__version__)\n","print(keras.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.0\n","2.2.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zl5UxAIAL3Ha","colab_type":"text"},"source":["Определим класс для загрузки наших данных:"]},{"cell_type":"code","metadata":{"id":"AeMyk7MO5ubs","colab_type":"code","colab":{}},"source":["class DatasetLoader:\n","\n","    def __init__(self, path_to_data_dir):\n","        self._path_to_data_dir = path_to_data_dir\n","        self._data_files = sorted(os.listdir(self._path_to_data_dir))\n","\n","\n","    def load_dataset(self):\n","        sentences = []\n","        for data_file in tqdm(self._data_files, desc='Loading data'):\n","            if not data_file.endswith('.csv'):\n","              continue\n","            with open(os.path.join(self._path_to_data_dir, data_file), 'r') as data_f:\n","                reader = csv.DictReader(data_f)\n","                sentence = []\n","                for row in reader:\n","                    sentence.append((row['token'], row['tag']))\n","                sentences.append(sentence)\n","        return sentences\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yErEXMp6MJ3l","colab_type":"text"},"source":["Определим класс для векторизации данных:"]},{"cell_type":"code","metadata":{"id":"zcewqi5c588E","colab_type":"code","colab":{}},"source":["class Vectorizer:\n","\n","    def __init__(self):\n","        self._max_sentence_len = 1000\n","        self._max_wordform_len = 30\n","        self._all_chars = u'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM,.?!:;\"«»-—1234567890'\n","        self._punct = u',.?!:;\"«»-—'\n","        self._letters = u'qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM'\n","        self._numbers = u'1234567890'\n","        self._len_all_chars = len(self._letters) + 2\n","        \n","        self._tag2id = {'O': 0, 'B-TIME': 1, 'I-TIME': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-DURATION': 5, 'I-DURATION': 6,\n","                        'B-SET': 7, 'I-SET': 8}\n","    \n","    def get_len_all_chars(self):\n","      return self._len_all_chars\n","\n","    def vectorize_chars_and_tokens(self, sentences):\n","        X_chars = self.vectorize_chars_dataset(sentences)\n","        X_sentences = self.vectorize_sentences(sentences)\n","        return X_chars, X_sentences\n","\n","    def vectorize_targets(self, sentences):\n","        y = [[self._tag2id[w[1]] for w in s] for s in sentences]\n","        y = pad_sequences(maxlen=self._max_sentence_len, sequences=y, padding=\"post\", value=self._tag2id[\"O\"])\n","        y = y.reshape(y.shape[0], y.shape[1], 1)\n","        return y\n","\n","    def vectorize_chars_dataset(self, sentences):\n","        X_chars = [[self._vectorize_chars_wordform(w[0]) for w in s] for s in sentences]\n","        X_chars = pad_sequences(maxlen=self._max_sentence_len, sequences=X_chars, padding=\"post\",\n","                                value=np.zeros((self._max_wordform_len, self._len_all_chars), dtype=np.int32))\n","        return X_chars\n","\n","    def _vectorize_chars_wordform(self, wordform):\n","        vector = np.zeros(self._max_wordform_len * self._len_all_chars)\n","        for i in range(len(wordform)):\n","            if i == self._max_wordform_len:\n","                break\n","            \"\"\"\n","            if wordform[i] in self._all_chars:\n","                ind = self._all_chars.index(wordform[i])\n","                vector[i * self._len_all_chars + ind] = 1.0\n","            \"\"\"\n","            # try to make 3 classes\n","            if wordform[i] in self._punct:\n","              ind = len(self._letters) + 1\n","              vector[i * self._len_all_chars + ind] = 1.0              \n","            if wordform[i] in self._letters:\n","              ind = self._letters.index(wordform[i])\n","              vector[i * self._len_all_chars + ind] = 1.0\n","            if wordform[i] in self._numbers:  \n","              ind = len(self._letters) + 2\n","              vector[i * self._len_all_chars + ind] = 1.0\n","        vector = vector.reshape((self._max_wordform_len, self._len_all_chars))\n","        return vector\n","\n","    def vectorize_sentences(self, sentences):\n","        X = [[w[0] for w in s] for s in sentences]\n","        new_X = []\n","        for seq in X:\n","            new_seq = []\n","            for i in range(self._max_sentence_len):\n","                try:\n","                    new_seq.append(seq[i])\n","                except:\n","                    new_seq.append(\"__PAD__\")\n","            new_X.append(new_seq)\n","        X = new_X\n","        return np.array(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Up-mBrVpNBdj","colab_type":"text"},"source":["Определим класс для обучения модели:"]},{"cell_type":"code","metadata":{"id":"7ihV_UTW6MYV","colab_type":"code","colab":{}},"source":["class Trainer:\n","\n","    def __init__(self):\n","        data_dir = os.path.join(time_dir, 'train')\n","        self._data_loader = DatasetLoader(data_dir)\n","        self._vectorizer = Vectorizer()\n","\n","        self._sentences = self._data_loader.load_dataset()\n","        self._val_sentences = self._sentences[-50:]\n","        self._train_samples = self._sentences[:200]\n","\n","        self._batch_size = 5\n","        self._max_len = 1000\n","        self.steps_per_epoch = len(self._train_samples)/self._batch_size\n","\n","        sess = tf.Session()\n","        K.set_session(sess)\n","\n","        print('Loading elmo model...')\n","        self._elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n","        print('Elmo model is loaded')\n","        sess.run(tf.global_variables_initializer())\n","        sess.run(tf.tables_initializer())\n","\n","\n","        self._model = self._define_model()\n","      \n","    def ElmoEmbedding(self, x):\n","        return self._elmo_model(inputs={\n","            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n","            \"sequence_len\": tf.constant(self._batch_size * [self._max_len])\n","        },\n","            signature=\"tokens\",\n","            as_dict=True)[\"elmo\"]\n","\n","    def _define_model(self):\n","        input_chars = Input(shape=(self._max_len, 30, self._vectorizer.get_len_all_chars()))\n","        chars = TimeDistributed(Bidirectional(LSTM(units=150,\n","                                                   recurrent_dropout=0.5)))(input_chars)\n","  \n","        input_text = Input(shape=(self._max_len,), dtype=\"string\")\n","        embedding = Lambda(self.ElmoEmbedding, output_shape=(self._max_len, 1024))(input_text)\n","        x = Bidirectional(LSTM(units=512, return_sequences=True,\n","                               recurrent_dropout=0.2, dropout=0.2))(embedding)\n","        x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n","                                   recurrent_dropout=0.2, dropout=0.2))(x)\n","\n","        x = concatenate([x_rnn, chars])  # residual connection to the first biLSTM\n","        crf = CRF(9, sparse_target=True)  # CRF layer\n","        out = crf(x)  # output\n","\n","        model = Model([input_chars, input_text], out)\n","        model.summary()\n","\n","        model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n","        return model\n","\n","    def _generate_train_samples(self):\n","      i = 0\n","      while True:\n","        X_chars, X_sentences = self._vectorizer.vectorize_chars_and_tokens(self._train_samples[i:i + self._batch_size])\n","        y = self._vectorizer.vectorize_targets(self._train_samples[i:i + self._batch_size])\n","        i += self._batch_size\n","        yield [np.array(X_chars), np.array(X_sentences)], np.array(y)\n","        if i == len(self._train_samples):\n","          i = 0\n","\n","    def _generate_val_samples(self):\n","      i = 0\n","      while True:\n","        X_chars, X_sentences = self._vectorizer.vectorize_chars_and_tokens(self._val_sentences[i:i + self._batch_size])\n","        y = self._vectorizer.vectorize_targets(self._val_sentences[i:i + self._batch_size])\n","        i += self._batch_size\n","        yield [np.array(X_chars), np.array(X_sentences)], np.array(y)\n","        if i == len(self._val_sentences):\n","          i = 0\n","\n","    def train(self):\n","        weights_dir = '/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya'\n","        if not os.path.exists(weights_dir):\n","          os.makedirs(weights_dir)\n","        weights_file = 'weights_chars_crf_' + path_to_exp + '_.h5'\n","        modelPath = os.path.join(weights_dir, weights_file)\n","        saver = ModelCheckpoint(modelPath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","        stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n","        history = self._model.fit_generator(self._generate_train_samples(), epochs=120,\n","                            validation_data=self._generate_val_samples(), steps_per_epoch=self.steps_per_epoch, validation_steps=10,\n","                            verbose=1, callbacks=[saver, stopper])\n","        return history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJSQXKqA7xgl","colab_type":"code","outputId":"dbe5b606-e2ae-4787-b2d7-79c24849e52e","executionInfo":{"status":"ok","timestamp":1591097564811,"user_tz":-420,"elapsed":7241646,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["trainer = Trainer()\n","trainer.train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data: 100%|██████████| 256/256 [03:20<00:00,  1.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loading elmo model...\n","Elmo model is loaded\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:72: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:72: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:515: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:515: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4048: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4048: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 1000)         0                                            \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 1000, 1024)   0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, 1000, 1024)   6295552     lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 1000, 30, 54) 0                                            \n","__________________________________________________________________________________________________\n","bidirectional_3 (Bidirectional) (None, 1000, 1024)   6295552     bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 1000, 300)    246000      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 1000, 1324)   0           bidirectional_3[0][0]            \n","                                                                 time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","crf_1 (CRF)                     (None, 1000, 9)      12024       concatenate_1[0][0]              \n","==================================================================================================\n","Total params: 12,849,128\n","Trainable params: 12,849,128\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:782: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n","  warnings.warn('CRF.loss_function is deprecated '\n","/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n","  warnings.warn('CRF.accuracy is deprecated and it '\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:782: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:984: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:984: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:971: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:971: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/120\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:172: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:172: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:188: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:188: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:204: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:204: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["40/40 [==============================] - 347s 9s/step - loss: 0.1745 - crf_viterbi_accuracy: 0.9619 - val_loss: 0.0831 - val_crf_viterbi_accuracy: 0.9813\n","\n","Epoch 00001: val_loss improved from inf to 0.08307, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 2/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0554 - crf_viterbi_accuracy: 0.9873 - val_loss: 0.0550 - val_crf_viterbi_accuracy: 0.9866\n","\n","Epoch 00002: val_loss improved from 0.08307 to 0.05501, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 3/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0423 - crf_viterbi_accuracy: 0.9894 - val_loss: 0.0462 - val_crf_viterbi_accuracy: 0.9877\n","\n","Epoch 00003: val_loss improved from 0.05501 to 0.04621, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 4/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0352 - crf_viterbi_accuracy: 0.9909 - val_loss: 0.0438 - val_crf_viterbi_accuracy: 0.9892\n","\n","Epoch 00004: val_loss improved from 0.04621 to 0.04381, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 5/120\n","40/40 [==============================] - 332s 8s/step - loss: 0.0301 - crf_viterbi_accuracy: 0.9921 - val_loss: 0.0387 - val_crf_viterbi_accuracy: 0.9904\n","\n","Epoch 00005: val_loss improved from 0.04381 to 0.03867, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 6/120\n","40/40 [==============================] - 332s 8s/step - loss: 0.0255 - crf_viterbi_accuracy: 0.9932 - val_loss: 0.0392 - val_crf_viterbi_accuracy: 0.9909\n","\n","Epoch 00006: val_loss did not improve from 0.03867\n","Epoch 7/120\n","40/40 [==============================] - 331s 8s/step - loss: 0.0218 - crf_viterbi_accuracy: 0.9943 - val_loss: 0.0613 - val_crf_viterbi_accuracy: 0.9840\n","\n","Epoch 00007: val_loss did not improve from 0.03867\n","Epoch 8/120\n","40/40 [==============================] - 332s 8s/step - loss: 0.0219 - crf_viterbi_accuracy: 0.9939 - val_loss: 0.0329 - val_crf_viterbi_accuracy: 0.9915\n","\n","Epoch 00008: val_loss improved from 0.03867 to 0.03292, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 9/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0181 - crf_viterbi_accuracy: 0.9949 - val_loss: 0.0346 - val_crf_viterbi_accuracy: 0.9933\n","\n","Epoch 00009: val_loss did not improve from 0.03292\n","Epoch 10/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0168 - crf_viterbi_accuracy: 0.9956 - val_loss: 0.0430 - val_crf_viterbi_accuracy: 0.9895\n","\n","Epoch 00010: val_loss did not improve from 0.03292\n","Epoch 11/120\n","40/40 [==============================] - 334s 8s/step - loss: 0.0180 - crf_viterbi_accuracy: 0.9955 - val_loss: 0.0320 - val_crf_viterbi_accuracy: 0.9928\n","\n","Epoch 00011: val_loss improved from 0.03292 to 0.03198, saving model to /content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_120_5_3_150_chars_elmo_2_.h5\n","Epoch 12/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0138 - crf_viterbi_accuracy: 0.9960 - val_loss: 0.0340 - val_crf_viterbi_accuracy: 0.9919\n","\n","Epoch 00012: val_loss did not improve from 0.03198\n","Epoch 13/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0131 - crf_viterbi_accuracy: 0.9961 - val_loss: 0.0521 - val_crf_viterbi_accuracy: 0.9886\n","\n","Epoch 00013: val_loss did not improve from 0.03198\n","Epoch 14/120\n","40/40 [==============================] - 334s 8s/step - loss: 0.0142 - crf_viterbi_accuracy: 0.9958 - val_loss: 0.0331 - val_crf_viterbi_accuracy: 0.9925\n","\n","Epoch 00014: val_loss did not improve from 0.03198\n","Epoch 15/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0097 - crf_viterbi_accuracy: 0.9969 - val_loss: 0.0387 - val_crf_viterbi_accuracy: 0.9904\n","\n","Epoch 00015: val_loss did not improve from 0.03198\n","Epoch 16/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0082 - crf_viterbi_accuracy: 0.9975 - val_loss: 0.0348 - val_crf_viterbi_accuracy: 0.9926\n","\n","Epoch 00016: val_loss did not improve from 0.03198\n","Epoch 17/120\n","40/40 [==============================] - 333s 8s/step - loss: 0.0058 - crf_viterbi_accuracy: 0.9977 - val_loss: 0.0408 - val_crf_viterbi_accuracy: 0.9901\n","\n","Epoch 00017: val_loss did not improve from 0.03198\n","Epoch 18/120\n","40/40 [==============================] - 334s 8s/step - loss: 0.0051 - crf_viterbi_accuracy: 0.9983 - val_loss: 0.0533 - val_crf_viterbi_accuracy: 0.9905\n","\n","Epoch 00018: val_loss did not improve from 0.03198\n","Epoch 19/120\n","40/40 [==============================] - 334s 8s/step - loss: 0.0058 - crf_viterbi_accuracy: 0.9980 - val_loss: 0.0379 - val_crf_viterbi_accuracy: 0.9917\n","\n","Epoch 00019: val_loss did not improve from 0.03198\n","Epoch 20/120\n","40/40 [==============================] - 335s 8s/step - loss: 0.0043 - crf_viterbi_accuracy: 0.9987 - val_loss: 0.0465 - val_crf_viterbi_accuracy: 0.9922\n","\n","Epoch 00020: val_loss did not improve from 0.03198\n","Epoch 21/120\n","40/40 [==============================] - 335s 8s/step - loss: 0.0073 - crf_viterbi_accuracy: 0.9977 - val_loss: 0.0527 - val_crf_viterbi_accuracy: 0.9884\n","\n","Epoch 00021: val_loss did not improve from 0.03198\n","Epoch 00021: early stopping\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7efc056b6630>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"QmJ39h4A2WQU","colab_type":"text"},"source":["Теперь давайте попробуем найти временные выражения, используя обученную модель. Для этого напишем класс **Predictor**"]},{"cell_type":"code","metadata":{"id":"QRiid9hLpDax","colab_type":"code","colab":{}},"source":["from nltk.tokenize import WordPunctTokenizer\n","\n","\n","class Predictor:\n","\n","    def __init__(self):\n","        self._vectorizer = Vectorizer()\n","        self._trainer = Trainer()\n","        self._tokenizer = WordPunctTokenizer()\n","        self._model = self._trainer._model\n","        self._model.load_weights('/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/weights_nastya/weights_chars_crf_' + path_to_exp + '_.h5')\n","        self._tag2id = {'O': 0, 'B-TIME': 1, 'I-TIME': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-DURATION': 5, 'I-DURATION': 6,\n","                        'B-SET': 7, 'I-SET': 8}\n","        self._id2tag = self._get_id2tag()\n","\n","    def _get_id2tag(self):\n","        id2tag = dict()\n","        for tag, id in self._tag2id.items():\n","            id2tag[id] = tag\n","        return id2tag\n","\n","    def _process_input_sentence(self, sentence):\n","        tokens = self._tokenizer.tokenize(sentence)\n","        tokens_to_process = [(token, 'O') for token in tokens]\n","        input_tokens = []\n","        for i in range(5):\n","            input_tokens.append(tokens_to_process)\n","        X_chars, X_sequence = self._vectorizer.vectorize_chars_and_tokens(sentences=input_tokens)\n","        return X_chars, X_sequence, tokens\n","\n","    def predict(self, text):\n","        X_chars, X_sequence, tokens = self._process_input_sentence(text)\n","        predicts = self._model.predict([X_chars, X_sequence])[0]\n","        result = []\n","        for i, token in enumerate(tokens):\n","            tag = self._id2tag[np.argmax(predicts[i])]\n","            result.append((token, tag))\n","        return result\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMNqOVNvp4S1","colab_type":"code","outputId":"083258f2-1b83-4227-9628-fb7ca67da1c3","executionInfo":{"status":"ok","timestamp":1590925002578,"user_tz":-420,"elapsed":12581,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":630}},"source":["predictor = Predictor()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data: 100%|██████████| 256/256 [00:00<00:00, 412.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loading elmo model...\n","Elmo model is loaded\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            (None, 1000)         0                                            \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 1000, 1024)   0           input_6[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_8 (Bidirectional) (None, 1000, 1024)   6295552     lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 1000, 30, 54) 0                                            \n","__________________________________________________________________________________________________\n","bidirectional_9 (Bidirectional) (None, 1000, 1024)   6295552     bidirectional_8[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 1000, 300)    246000      input_5[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 1000, 1324)   0           bidirectional_9[0][0]            \n","                                                                 time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","crf_3 (CRF)                     (None, 1000, 9)      12024       concatenate_3[0][0]              \n","==================================================================================================\n","Total params: 12,849,128\n","Trainable params: 12,849,128\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n","  warnings.warn('CRF.loss_function is deprecated '\n","/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n","  warnings.warn('CRF.accuracy is deprecated and it '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zAx_mcUtqAcg","colab_type":"code","outputId":"fbfab0b0-e33c-475f-f2ad-26739f31062e","executionInfo":{"status":"ok","timestamp":1590925032436,"user_tz":-420,"elapsed":8694,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["text = 'he was born in 1994'\n","result = predictor.predict(text)\n","for r in result:\n","    print(r)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('he', 'O')\n","('was', 'O')\n","('born', 'O')\n","('in', 'O')\n","('1994', 'B-DATE')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TxHLKhwOXbve","colab_type":"text"},"source":["Определим класс для оценки качества извлечения временных выражений"]},{"cell_type":"code","metadata":{"id":"vnCFGv-nYrDa","colab_type":"code","outputId":"aec22bd4-7e77-406c-f0f4-14f94374cd18","executionInfo":{"status":"ok","timestamp":1590925069964,"user_tz":-420,"elapsed":7521,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":502}},"source":["!pip install seqeval\n","from seqeval.metrics import classification_report"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n","Collecting Keras>=2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.7)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=5a7ff659ce1d72e1614cce0328abfee30baa7b3b3957eab2502a929a3e31147d\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","Successfully built seqeval\n","Installing collected packages: Keras, seqeval\n","  Found existing installation: Keras 2.2.2\n","    Uninstalling Keras-2.2.2:\n","      Successfully uninstalled Keras-2.2.2\n","Successfully installed Keras-2.3.1 seqeval-0.0.12\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"czfRqLBIU-eS","colab_type":"code","colab":{}},"source":["class Evaluator:\n","    \"\"\" Класс для оценки качества извлечения временных выражений \"\"\"\n","\n","    def __init__(self, file_dir, column_pred):\n","        self._predicted_files_dir = os.path.join('/content/gdrive/My Drive/Mezentseva Zavarzina/experiments_dl_model/test_ml/', path_to_exp)\n","        self._predicted_files = sorted(os.listdir(self._predicted_files_dir))\n","        self.column_pred = column_pred\n","\n","    def evaluate(self) -> str:\n","        \"\"\" Оценка качества извлечения временных выражений\n","        :return: метрики: precision, recall, F-1 score для каждого класса отдельно и для всех усреднённые\n","        \"\"\"\n","        preds, targets = self._load_predictons()\n","        report = classification_report(y_true=targets, y_pred=preds)\n","        return report\n","\n","    def _load_predictons(self):\n","        \"\"\" Загружает targets и predictions из заранее сформированных csv файлов\n","        :return: predictions, targets\n","        \"\"\"\n","        preds = []\n","        targets = []\n","        for predicted_file in self._predicted_files:\n","            with open(os.path.join(self._predicted_files_dir, predicted_file), 'r') as predicted_file:\n","                reader = csv.DictReader(predicted_file)\n","                preds_from_file = []\n","                targets_from_file = []\n","                for row in reader:\n","                    preds_from_file.append(row[str(self.column_pred)])\n","                    targets_from_file.append(row['tag'])\n","                preds.append(preds_from_file)\n","                targets.append(targets_from_file)\n","        return preds, targets\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LE5a78EHaBbS","colab_type":"text"},"source":["Разметка тестового датасета,  получение метрик"]},{"cell_type":"code","metadata":{"id":"nBSDr509a3JS","colab_type":"code","colab":{}},"source":["class Labeler:\n","\n","  def __init__(self, path_to_data_dir):\n","        self._path_to_data_dir = path_to_data_dir\n","        self._data_files = sorted(os.listdir(self._path_to_data_dir))\n","\n"," \n"," \n","  def label_dataset(self, column_name):\n","        \"\"\"\n","        функция, которая записывает новую информацию в датасет\n","        :param column_name: имя столбца\n","        \"\"\"\n","        for data_file in tqdm(self._data_files, desc='Adding data'):\n","            path_to_test_file = os.path.join(self._path_to_data_dir, data_file)\n","            print(1)\n","            with open(path_to_test_file, 'r') as data_f:\n","                reader = csv.DictReader(data_f)\n","                print(2)\n","                test_dir = result_dir + path_to_exp\n","                with open(os.path.join(test_dir, data_file), 'w', encoding='utf-8', newline='') as task:\n","                    print('ok')                  \n","                    fieldnames = ['token', 'tag', column_name]\n","                    writer = csv.DictWriter(task, fieldnames=fieldnames)\n","                    writer.writeheader()\n","                    predictions = list() \n","                    data = list()\n","                    sentence = list()\n","                    for row in reader:\n","                      data.append((row['token'], row['tag']))\n","                      sentence.append(row['token'])\n","                    predictions = predictor.predict(' '.join(sentence))\n","                    for i, elem in enumerate(predictions):\n","                      token, tag = data[i]\n","                      token, pred_tag = elem \n","                      writer.writerow({'token' : token, 'tag' : tag, column_name: pred_tag})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE2ebYfTa3df","colab_type":"code","outputId":"ade11cd4-620d-4fa8-c1ba-00589c2cc517","executionInfo":{"status":"ok","timestamp":1590927333231,"user_tz":-420,"elapsed":123248,"user":{"displayName":"Анастасия Алексеевна Мезенцева","photoUrl":"","userId":"07144606596434247568"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["ml_label = Labeler(os.path.join(time_dir, 'test'))\n","tag = 'predicted_ml_tag'\n","ml_label.label_dataset(tag)\n","evaluator = Evaluator(os.path.join(time_dir, 'test'), tag)\n","report = evaluator.evaluate()\n","print(report)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Adding data:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:   5%|▌         | 1/20 [00:06<01:57,  6.20s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  10%|█         | 2/20 [00:12<01:51,  6.17s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  15%|█▌        | 3/20 [00:18<01:44,  6.17s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  20%|██        | 4/20 [00:24<01:38,  6.16s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  25%|██▌       | 5/20 [00:30<01:32,  6.15s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  30%|███       | 6/20 [00:36<01:25,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  35%|███▌      | 7/20 [00:42<01:19,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  40%|████      | 8/20 [00:49<01:13,  6.14s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  45%|████▌     | 9/20 [00:55<01:07,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  50%|█████     | 10/20 [01:01<01:01,  6.12s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  55%|█████▌    | 11/20 [01:07<00:54,  6.11s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  60%|██████    | 12/20 [01:13<00:48,  6.11s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  65%|██████▌   | 13/20 [01:19<00:42,  6.11s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  70%|███████   | 14/20 [01:25<00:36,  6.12s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  75%|███████▌  | 15/20 [01:31<00:30,  6.12s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  80%|████████  | 16/20 [01:38<00:24,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  85%|████████▌ | 17/20 [01:44<00:18,  6.15s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  90%|█████████ | 18/20 [01:50<00:12,  6.14s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data:  95%|█████████▌| 19/20 [01:56<00:06,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","ok\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Adding data: 100%|██████████| 20/20 [02:02<00:00,  6.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["           precision    recall  f1-score   support\n","\n","     DATE       0.83      0.68      0.75       101\n"," DURATION       0.41      0.51      0.46        35\n","      SET       0.00      0.00      0.00         4\n","     TIME       0.50      0.25      0.33         4\n","\n","micro avg       0.68      0.61      0.64       144\n","macro avg       0.70      0.61      0.65       144\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"65qSzBkmZkGP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}